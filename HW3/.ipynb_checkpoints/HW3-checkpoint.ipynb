{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erin Witmer  \n",
    "CSC 440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.1] Suppose you have the set C of all frequent closed itemsets on a data set D, as well as the support count for each frequen closed itemset. Describe an algorithm to determine whether a given itemset X is frequent or not, and the support of X if it is frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset X is closed in set D if there exists no superset Y such than Y has the same support count as X in D. Per the text: \"The set of closed frequent itemsets contains complete information regarding the frequent itemsets.\" A simple algorithm to determine whether X is frequent or not and the support is:\n",
    "\n",
    "``` \n",
    "sort set C by support count (high to low)\n",
    "\n",
    "for each itemset (i) in C:\n",
    "    \n",
    "    if X is a subset of C(i):\n",
    "       return support count for this item \n",
    "            \n",
    "    else:\n",
    "       next item\n",
    "    \n",
    "return X is not frequent if X is not a subset of any itemset in C \n",
    "```\n",
    "We know that the support count of X if it is a subset of the itemset in C is the same as the support count in that itemset based on the definition of the closed itemset. We know it is frequent because C is a set of all frequent closed itemsets. We know that if X is not a subset of any itemset in C, it is not frequent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.3] The Apriori algorithm makes use of prior knowledge of subset support properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Prove that all nonempty subsets of a frequent itemset must also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset (I) is a frequent itemset if the percentage of transactions (T) in a database (D) containing that itemset (I) meets or exceeds a minimum support threshold (min_sup). That is: $$ \\frac{countOfTransactionsContaining(I)}{totalTransactionsIn(D)} \\geq minSupport \\\\$$if $$ A \\subset I\n",
    "$$ then the count of transactions containing A is greater than or equal to the count of transactions containing I, because every transaction containing I, will also contain A. This is true for all non-empty subsets of I. The denominator, the count of all transactions, does not change. So if the numerator (count of transactions containing A) is greater than or equal to the count of transactions containing I, and the denominator is constant, then the support level will be greater than or equal to the support level of I. If I is frequent, or higher than the min support level, then A will also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Prove that the support of any nonempty subset s′ of itemset s must be at least as great as the support of s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support count of s is the number of times an itemset (s) appears in the universe of all transactions. For example, if s = {beer, nuts, diapers} then the support count of s is the number of transactions which contain all three of those items. Any subset of s, for example, s' = {beer, nuts} will occur in all transactions in which s occurs. In other words, any transaction which contains {beer, nuts, diapers} will also contain all subsets of that itemset {beer, nuts}. The support count of s' will therefore be at least as high as s, and may be higher if the subset s' occurs in additional transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Given frequent itemset l and subset s of l, prove that the confidence of the rule “s′ ⇒(l−s′)” cannot be more than the confidence of “s⇒(l−s),” where s′ is a subset of s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ confidence(s'⇒l-s')= \\frac{supportCount(s' ∪ (l-s'))}{supportCount(s')}$$\n",
    "\n",
    "$$confidence(s⇒l-s)= \\frac{supportCount(s ∪ (l-s))}{supportCount(s)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know s' is a subset of s and therefore a subset of l, l-s' represents the complement to the frequent itemset, l. In union with s', it equals the frequent itemset. So the count of s' ∪ (l-s') will be the support count of l. Similarly, l-s represents the complement to the frequent itemset, and the count of s ∪ (l-s) will be the count of l. So we know the numerators in the equations above are the same. As proved above, the support count of s' as a subset of s will always be greater than or equal to the support count of s. So the denominator in the first equation above will always be greater than or equal to the denominator in the second equation, and the value of the first equation will always be less than or equal to the value of the second equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) A partitioning variation of Apriori subdivides the transactions of a database D into n nonoverlapping partitions. Prove that any itemset that is frequent in D must be frequent in at least one partition of D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a given itemset is frequent in D, then it must be locally frequent in at least one nonoverlapping partition of D. The itemset is frequent in D if:\n",
    "$$ \\frac{supportCount(I)}{totalTransactionsIn(D)} \\geq minSupport $$\n",
    "If the database is divided up, at least one of the partitions will have to meet the minimum support for that local partition because if no partition did, the global support count needed would not be reached. For example, if there are 100 transactions, and the min_support is 60%, then there must be 60 instances of the itemset in the database. If this was partitioned into 5 sets of 20 transactions, then the min_support count would be 12 for each partition. If C1...C5 < 12, then the total support count would be < 60, and the itemset would not be frequent. Therefore, at least one of the partitions needs to be locally frequent if the itemset is frequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.4] Let c be a candidate itemset in Ck generated by the Apriori algorithm. How many length-(k − 1) subsets do we need to check in the prune step? Per your previous answer, can you give an improved version of procedure has infrequent subset in Figure 6.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every candidate C(k) generated, we need to check k choose k-1 subsets, which is equal to k subsets. \n",
    "$$ {k \\choose k-1} = k$$\n",
    "For example, if C(k) = {I1, I2, I3}, we need to check 3 choose 2, or 3 itemsets: {I1, I2}, {I1, I3}, {I2, I3}. One way to improve the procedure ```has_infrequent_subset()``` is to assume that the items which constructed the candidate set are frequent, so we don't need to check them.  So for example, (k-1) frequent itemsets ABC + ABD = ABCD (Candidate of k length). Pruning combinations to check are ABC, ABD, BCD, ACD but it was constructed from ABC, ABD, so those are assumed frequent, you only have to check BCD and ACD or (k-2) sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.5] Section 6.2.2 describes a method for generating association rules from frequent itemsets. Propose a more efficient method. Explain why it is more efficient than the one proposed there. (Hint: Consider incorporating the properties of Exercises 6.3(b), (c) into your design.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section proposes the following method for generating association rules from frequent itemsets: \n",
    "- For each frequent itemset l, generate all nonempty subsets of l.\n",
    "- For every nonempty subset s of l, output the rule “s ⇒ (l − s)” if support count(l) ≥ min conf, where min conf is the minimum confidence threshold.\n",
    "\n",
    "While in general, confidence does not have an anti-monotone property\n",
    "c(ABC →D) can be larger or smaller than c(AB →D); confidence of rules generated from the same itemset do have an anti-monotone property. So the example provided in the text: frequent itemset X = {I1, I2, I5}, the resulting association rules:\n",
    "\n",
    "| Rule generated | Confidence |\n",
    "| :--- | ---- |\n",
    "|{I1, I2} ⇒ I5 | confidence = 2/4 = 50% |\n",
    "|{I1, I5} ⇒ I2 | confidence = 2/2 = 100% |\n",
    "|{I2, I5} ⇒ I1 | confidence = 2/2 = 100% |\n",
    "|I1 ⇒ {I2, I5} | confidence = 2/6 = 33% |\n",
    "|I2 ⇒ {I1, I5} | confidence = 2/7 = 29% |\n",
    "|I5 ⇒ {I1, I2} | confidence = 2/2 = 100% |\n",
    "\n",
    "The min confidence is 70%. The first rule generated {I1, I2} ⇒ I5 doesn't meet that threshold. Since confidence is anti-monotone with respect to the right side of the rule, we know that any of the rules containing I5 on the right side will not meet the min confidence threshold either. So we didn't need to check the two rules that contain I5 on the right: I1 ⇒ {I2, I5} and I2 ⇒ {I1, I5} as a result of the anti-monotone property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.11] Most frequent pattern mining algorithms consider only distinct items in a transaction. However, multiple occurrences of an item in the same shopping basket, such as four cakes and three jugs of milk, can be important in transactional data analysis. How can one mine frequent itemsets efficiently considering multiple occurrences of items? Propose modifications to the well-known algorithms, such as Apriori and FP-growth, to adapt to such a situation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A simple way to handle this in both the Apriori and FP-growth algorithm is to treat each instance of a duplicate item as a unique item. So rather than compressing a transaction with 4 cakes and 3 jugs of milk to {cake, milk} the transaction would be: {cake_1, cake_2, cake_3, cake_4, milk_1, milk_2, milk_3}. The benefit of this analysis would be you could analyze association rules like, if you buy 2 cartons of eggs, how likely are you to buy 2 cartons of milk? The problem with this is that it dramatically increases the computational complexity of the analysis because the number of unique items increases. Given d itemsets, the total number of itemsets = 2^d, and the total possible association rules = 3^d + 2^(d+1) + 1. So any increase in d will increase the computational complexity of the analysis exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"Remove duplicates in transactions.\n",
    "\n",
    "    Args:\n",
    "      data: A dict of {key: transactionID, value: transaction items}.\n",
    "    Raises:\n",
    "      TypeError: If the data is not of type dict.\n",
    "    Returns:\n",
    "      The data with only unique items in each transaction.\n",
    "    \"\"\"\n",
    "    if (type(data)) != dict:\n",
    "        raise TypeError('Data needs to be in dictionary format')\n",
    "        \n",
    "    for key, value in data.items():\n",
    "        data[key] = list(set(value))\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_min_support(data, support_pct):\n",
    "    \"\"\"Returns minimum support count of a data set.\n",
    "\n",
    "    Args:\n",
    "      data: A dict of {key: transactionID, value: transaction items}.\n",
    "      support_pct: minimum support required to be defined as frequent.\n",
    "    Raises:\n",
    "      TypeError: If the data is not of type dict.\n",
    "      ValueErorr: If the support_pct is outside of the range [0,1]\n",
    "    Returns:\n",
    "      The support count required to meet the threshold of frequent itemset.\n",
    "    \"\"\"\n",
    "    if (type(data)) != dict:\n",
    "        raise TypeError('Data needs to be in dictionary format')\n",
    "        \n",
    "    if support_pct > 1 or support_pct < 0:\n",
    "        raise ValueError('support_pct must be in the range [0,1]')\n",
    "        \n",
    "    return math.ceil(len(data) * support_pct)\n",
    "\n",
    "\n",
    "def prune_itemsets(candidates, support_count):\n",
    "    \"\"\"Returns frequent itemsets based on support count.\n",
    "    Args:\n",
    "      candidates: A dict of {key: itemset, value: count}.\n",
    "      support_count: An integer based on number of transactions and support threshold\n",
    "    Raises:\n",
    "      TypeError: If the data is not of type dict.\n",
    "      ValueError: If the support_count is not a positive number\n",
    "    Returns:\n",
    "      The frequent itemset.\n",
    "    \"\"\" \n",
    "    if (type(candidates)) != dict:\n",
    "        raise TypeError('Data needs to be in dictionary format')\n",
    "        \n",
    "    if support_count < 0:\n",
    "        raise ValueError('support_count must be positive')\n",
    "        \n",
    "    remove = [key for key, value in candidates.items() if value < support_count]    \n",
    "    for k in remove:  # remove all items that don't meet support count\n",
    "        del candidates[k]\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_F1(data, support_count):\n",
    "    \"\"\"Returns frequent itemsets of L1.\n",
    "    Args:\n",
    "      data: A dict of {key: transactionID, value: transaction items}.\n",
    "    Raises:\n",
    "      TypeError: If the data is not of type dict.\n",
    "    Returns:\n",
    "      The count required to meet the threshold of frequent itemset.\n",
    "    \"\"\"    \n",
    "    if (type(data)) != dict:\n",
    "        raise TypeError('Data needs to be in dictionary format')\n",
    "    \n",
    "    if (type(support_count)) != int:\n",
    "        raise TypeError('support_count needs to be an integer')    \n",
    "        \n",
    "    freq_data = {}  # new dict to store item count    \n",
    "    \n",
    "    for key, value in data.items():  # for each item in the transaction        \n",
    "        for i in range(len(value)):   # if the key is in the new dict\n",
    "            if (value[i] in freq_data):               \n",
    "                freq_data[value[i]] += 1  # increment\n",
    "            else:          \n",
    "                freq_data[value[i]] = 1  # else add with count of one\n",
    "    \n",
    "    prune_itemsets(freq_data, support_count) \n",
    "    \n",
    "    return freq_data  # return the new dict\n",
    "\n",
    "\n",
    "def candidate_generation(freq_data):\n",
    "    \"\"\"Generate L(k+1) from Fk.\n",
    "    Args:\n",
    "      freq_data: A dict of frequent itemsets of length k {key: itemset, value: count}.\n",
    "    Raises:\n",
    "      TypeError: If the data is not of type dict.\n",
    "    Returns:\n",
    "      A list of the possible candidates (tuples) of length k+1.\n",
    "    \"\"\"     \n",
    "    if (type(freq_data)) != dict:\n",
    "        raise TypeError('Data needs to be in dictionary format')\n",
    "    \n",
    "    k_frequent = sorted(list(freq_data.keys())) # sort Lk freq itemset alphabetically\n",
    "    k1_candidates = []  # L(k+1) candidates    \n",
    "    \n",
    "    for i in range(len(k_frequent)):        \n",
    "        for j in range(i+1,len(k_frequent)):  # for every combination of current\n",
    "            if (k_frequent[i][:-1]==k_frequent[j][:-1]):  # if (k-1) == (k-1)\n",
    "                prefix = k_frequent[i]\n",
    "                suffix = k_frequent[j][-1]  # new candidate = k + k[-1]              \n",
    "                k1_candidates.append((*prefix, suffix))              \n",
    "    \n",
    "    return k1_candidates  # return k+1 candidates\n",
    "\n",
    "\n",
    "def get_all_subsets(candidate, k):\n",
    "    \"\"\"Generate all k-1 subsets of a candidate.\n",
    "    Args:\n",
    "      candidate: A tuple.\n",
    "      k: the length of the tuple\n",
    "    Raises:\n",
    "      TypeError: If candidates is not of type tuple.\n",
    "      TypeError: If k is not of type integer.\n",
    "    Returns:\n",
    "      A list length k of k-1 length subsets.\n",
    "    \"\"\"    \n",
    "    if (type(candidate)) != tuple:\n",
    "        raise TypeError('candidate needs to be a tuple')\n",
    "    \n",
    "    if (type(k)) != int:\n",
    "        raise TypeError('k needs to be an integer')\n",
    "    \n",
    "    return list(itertools.combinations(candidate,k-1))\n",
    "\n",
    "\n",
    "def has_infrequent_subset(subsets, freq_data):\n",
    "    \"\"\"Check if candidate as an infrequent subset of length k-1.\n",
    "    Args:\n",
    "      candidate: A list L(k-1) of subsets.\n",
    "      freq_data: freq_data: A dict of frequent itemsets.\n",
    "    Raises:\n",
    "      TypeError: If subsets is not of type list.\n",
    "      TypeError: If freq_data is not of type dict.\n",
    "    Returns:\n",
    "      True if the list contains an infrequent subset, False if it doesn't.\n",
    "    \"\"\" \n",
    "    for subset in subsets:\n",
    "        if subset not in freq_data:\n",
    "            return True\n",
    "    return False\n",
    " \n",
    "    \n",
    "def candidate_pruning(candidates, freq_data):\n",
    "    \"\"\"Prune candidate itemsets in L(k+1) containing subsets of L(k) that are infrequent.\n",
    "    Args:\n",
    "      candidates: A list of tuples of L(k+1).\n",
    "      freq_data: A dict of frequent itemsets of L(k)\n",
    "    Raises:\n",
    "      TypeError: If candidates is not of type list.\n",
    "      TypeError: If freq_data is not of type dict.\n",
    "    Returns:\n",
    "      A pruned list of the possible candidates (tuples) of length k+1.\n",
    "    \"\"\"\n",
    "    if (type(candidates)) != list:\n",
    "        raise TypeError('candidates needs to be in list format')\n",
    "    \n",
    "    if (type(freq_data)) != dict:\n",
    "        raise TypeError('freq_data needs to be in dict format')\n",
    "\n",
    "    k = len(candidates[0])\n",
    "    if k == 2:  # all L1 subsets are assumed frequent\n",
    "        return candidates\n",
    "    \n",
    "    for i in reversed(range(len(candidates))):  # for all candidates of length k\n",
    "        subsets = get_all_subsets(candidates[i], k)  # get all subsets of length k-1\n",
    "        if (has_infrequent_subset(subsets, freq_data)):  # if any subsets are not freq\n",
    "            candidates.remove(candidates[i])\n",
    "    \n",
    "    return candidates\n",
    " \n",
    "    \n",
    "def candidate_count(candidates, data):\n",
    "    \"\"\"Generate support count of all candidates length k post-prune.\n",
    "    Args:\n",
    "      candidates: A list of tuples of L(k).\n",
    "      data: A dict of transactions\n",
    "    Raises:\n",
    "      TypeError: If candidates is not of type list.\n",
    "      TypeError: If data is not of type dict.\n",
    "    Returns:\n",
    "      A dict of candidates of length k with support count.\n",
    "    \"\"\"\n",
    "    if (type(candidates)) != list:\n",
    "        raise TypeError('candidates needs to be in list format')\n",
    "    \n",
    "    if (type(data)) != dict:\n",
    "        raise TypeError('data needs to be in dict format')\n",
    " \n",
    "    data_count = {}\n",
    "    \n",
    "    for key, value in data.items():       \n",
    "        for candidate in candidates:\n",
    "            if set(candidate).issubset(set(value)):\n",
    "                if (candidate in data_count):\n",
    "                    data_count[candidate] += 1\n",
    "                else:\n",
    "                    data_count[candidate] = 1\n",
    "    return data_count\n",
    "\n",
    "\n",
    "def find_frequent_itemsets(data, support_pct):\n",
    "    \"\"\"Generate all frequent itemsets from a transaction database.\n",
    "    Args:\n",
    "      data: A dict of {key: transactionID, value: transaction items}.\n",
    "      support_pct: minimum support required to be defined as frequent.\n",
    "    Raises:\n",
    "      TypeError: If data is not of type dict.\n",
    "      ValueErorr: If the support_pct is outside of the range [0,1]\n",
    "    Returns:\n",
    "      A dict of all frequent itemsets.\n",
    "    \"\"\"    \n",
    "    if (type(data)) != dict:\n",
    "        raise TypeError('Data needs to be in dictionary format')\n",
    "        \n",
    "    if support_pct > 1 or support_pct < 0:\n",
    "        raise ValueError('support_pct must be in the range [0,1]')       \n",
    "    \n",
    "    data = remove_duplicates(data)  \n",
    "    support_count = get_min_support(data, support_pct)\n",
    "    \n",
    "    all_freq = {}\n",
    "    \n",
    "    frequent = generate_F1(data, support_count) # generate L1 freqent\n",
    "    all_freq.update(frequent)  # update dict\n",
    "    candidates = candidate_generation(frequent)  # generate L2 candidates\n",
    "    \n",
    "    while (candidates != []):  \n",
    "        candidates = candidate_pruning(candidates, frequent)  # prune candidates\n",
    "        count = candidate_count(candidates, data)  # support counting\n",
    "        frequent = prune_itemsets(count, support_count)  # candidate elimination\n",
    "        all_freq.update(frequent)  # update frequent itemset dict\n",
    "        candidates = candidate_generation(frequent)  # generate new candidates\n",
    "    \n",
    "    return all_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 5, 'E': 4, 'Y': 3, 'O': 3, 'M': 3, ('E', 'K'): 4, ('E', 'O'): 3, ('K', 'M'): 3, ('K', 'O'): 3, ('K', 'Y'): 3, ('E', 'K', 'O'): 3}\n"
     ]
    }
   ],
   "source": [
    "test = {'T100':['M','O','N','K','E','Y'],\n",
    "        'T200':['D','O','N','K','E','Y'],\n",
    "        'T300':['M','A','K','E'],\n",
    "        'T400':['M','U','C','K','Y'], \n",
    "        'T500':['C','O','O','K','I','E']}\n",
    "\n",
    "all_freq = find_frequent_itemsets(test, .6)\n",
    "print(all_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemset_difference(itemset, subset):\n",
    "    \"\"\"Generates a list of two tuples: [(s), (l-s)].\n",
    "    Args:\n",
    "      itemset: a frequent itemset.\n",
    "      subset: a subset of the frequent itemset.\n",
    "    Raises:\n",
    "      TypeError: If itemset is not of type tuple.\n",
    "      TypeError: If subset is not of type tuple.\n",
    "    Returns:\n",
    "      The list: [s, (l-s)].\n",
    "    \"\"\"    \n",
    "    if (type(itemset)) != tuple:\n",
    "        raise TypeError('itemset needs to be a tuple')\n",
    "    if (type(subset)) != tuple:\n",
    "        raise TypeError('subset needs to be a tuple')        \n",
    "        \n",
    "    diff = [tuple(sorted(subset)), tuple(sorted(set(itemset).difference(set(subset))))]\n",
    "    \n",
    "    if len(diff[0]) == 1:\n",
    "        diff[0] = ''.join(diff[0])\n",
    "\n",
    "    if len(diff[1]) == 1:\n",
    "        diff[1] = ''.join(diff[1])        \n",
    "    \n",
    "    return diff\n",
    "    \n",
    "def generate_candidate_rules(freq_itemset):\n",
    "    \"\"\"Generates a list of all rules candidates for an itemset: [(s), (l-s)].\n",
    "    Args:\n",
    "      freq_itemset: a frequent itemset in the form of a tuple.\n",
    "    Returns:\n",
    "      All possible rules for the itemset.\n",
    "    \"\"\"\n",
    "    k = len(freq_itemset) \n",
    "    all_rules = []\n",
    "    \n",
    "    for i in range(k-1,0,-1):  # loop through k choose i\n",
    "        combinations = list(itertools.combinations(freq_itemset, i))  # get all combinations\n",
    "        candidates = list(map(lambda x: itemset_difference(freq_itemset, x), combinations))\n",
    "        all_rules.extend(candidates)\n",
    "    \n",
    "    return all_rules\n",
    "\n",
    "def get_support_confidence(rule_set, support_count, length, all_frequent):\n",
    "    support = support_count / length\n",
    "    confidence = support_count / all_frequent[rule_set[1]]\n",
    "    \n",
    "    rule_set.extend([support, confidence])\n",
    "    return rule_set\n",
    "\n",
    "def prune_candidate_rules(freq_itemset, all_frequent, min_confidence, length):\n",
    "    \"\"\"Tests the candidates for one frequent itemset and prunes < min_confidence\n",
    "    Args:\n",
    "      freq_itemset: a frequent itemset in the form of a tuple.\n",
    "      all_frequent: a dict of all frequent itemsets\n",
    "      min_confidence: minimum confidence level\n",
    "    Raises:\n",
    "      TypeError: If all_frequent is not a dictionary.\n",
    "      ValueError: If freq_itemset is not of type tuple.\n",
    "    Returns:\n",
    "      All possible rules for the itemset.\n",
    "    \"\"\"    \n",
    "    \n",
    "    if (type(all_frequent)) != dict:\n",
    "        raise TypeError('freq_itemset needs to be in dictionary format')\n",
    "        \n",
    "    if min_confidence > 1 or min_confidence < 0:\n",
    "        raise ValueError('support_pct must be in the range [0,1]') \n",
    "        \n",
    "    support_count = all_frequent[freq_itemset]  # numerator is same for all rules\n",
    "    max_count = math.floor(support_count / min_confidence)  # max denominator\n",
    "    all_rules = generate_candidate_rules(freq_itemset)\n",
    "    pruned_rules = list(filter(lambda x: all_frequent[(x[1])] <= max_count, all_rules))\n",
    "    list(map(lambda x: get_support_confidence(x, support_count, length, all_frequent), pruned_rules))\n",
    "    \n",
    "    return pruned_rules\n",
    "\n",
    "    \n",
    "def generate_all_rules(all_frequent, min_confidence, length):\n",
    "    \n",
    "    associations = [] \n",
    "    for key, value in all_frequent.items():\n",
    "        pruned_rules = prune_candidate_rules(key, all_frequent, min_confidence, length)\n",
    "        associations.extend(pruned_rules)\n",
    "        \n",
    "    return associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['E', 'K', 0.8, 0.8],\n",
       " ['K', 'E', 0.8, 1.0],\n",
       " ['E', 'O', 0.6, 1.0],\n",
       " ['K', 'M', 0.6, 1.0],\n",
       " ['K', 'O', 0.6, 1.0],\n",
       " ['K', 'Y', 0.6, 1.0],\n",
       " [('E', 'K'), 'O', 0.6, 1.0],\n",
       " ['E', ('K', 'O'), 0.6, 1.0],\n",
       " ['K', ('E', 'O'), 0.6, 1.0]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_all_rules(all_freq, .8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
