{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erin Witmer  \n",
    "CSC 440  \n",
    "Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.1] Suppose you have the set C of all frequent closed itemsets on a data set D, as well as the support count for each frequen closed itemset. Describe an algorithm to determine whether a given itemset X is frequent or not, and the support of X if it is frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset X is closed in set D if there exists no superset Y such than Y has the same support count as X in D. Per the text: \"The set of closed frequent itemsets contains complete information regarding the frequent itemsets.\" A simple algorithm to determine whether X is frequent or not and the support is:\n",
    "\n",
    "``` \n",
    "sort set C by support count (high to low)\n",
    "\n",
    "for each itemset (i) in C:\n",
    "    \n",
    "    if X is a subset of C(i):\n",
    "       return support count for this item \n",
    "            \n",
    "    else:\n",
    "       next item\n",
    "    \n",
    "return X is not frequent if X is not a subset of any itemset in C \n",
    "```\n",
    "We know that the support count of X if it is a subset of the itemset in C is the same as the support count in that itemset based on the definition of the closed itemset. We know it is frequent because C is a set of all frequent closed itemsets. We know that if X is not a subset of any itemset in C, it is not frequent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.3] The Apriori algorithm makes use of prior knowledge of subset support properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Prove that all nonempty subsets of a frequent itemset must also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset (I) is a frequent itemset if the percentage of transactions (T) in a database (D) containing that itemset (I) meets or exceeds a minimum support threshold (min_sup). That is:  \n",
    "\n",
    "$$ \\frac{countOfTransactionsContaining(I)}{totalTransactionsIn(D)} \\geq minSupport \\\\$$if $$ A \\subset I\n",
    "$$ then the count of transactions containing A is greater than or equal to the count of transactions containing I, because every transaction containing I, will also contain A. This is true for all non-empty subsets of I. Support of an itemset never exceeds the support of its subsets. This is known as the anti-monotone property of support. The denominator, the count of all transactions, does not change. So if the numerator (count of transactions containing A) is greater than or equal to the count of transactions containing I, and the denominator is constant, then the support level will be greater than or equal to the support level of I. If I is frequent, or higher than the min support level, then A will also be frequent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Prove that the support of any nonempty subset s′ of itemset s must be at least as great as the support of s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this can be proven based on the anti-monotone property of support. Support of an itemset never exceeds the support of its subsets. The support count of s is the number of times an itemset (s) appears in the universe of all transactions. For example, if s = {beer, nuts, diapers} then the support count of s is the number of transactions which contain all three of those items. Any subset of s, for example, s' = {beer, nuts} will occur in all transactions in which s occurs. In other words, any transaction which contains {beer, nuts, diapers} will also contain all subsets of that itemset {beer, nuts}. The support count of s' will therefore be at least as high as s, and may be higher if the subset s' occurs in additional transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Given frequent itemset l and subset s of l, prove that the confidence of the rule “s′ ->(l−s′)” cannot be more than the confidence of “s->(l−s),” where s′ is a subset of s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ confidence(s'->l-s')= \\frac{supportCount(s' ∪ (l-s'))}{supportCount(s')}$$\n",
    "\n",
    "$$confidence(s->l-s)= \\frac{supportCount(s ∪ (l-s))}{supportCount(s)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know s' is a subset of s and therefore a subset of l, l-s' represents the complement to the frequent itemset, l. In union with s', it equals the frequent itemset. So the count of s' ∪ (l-s') will be the support count of l. Similarly, l-s represents the complement to the frequent itemset, and the count of s ∪ (l-s) will be the count of l. So we know the numerators in the equations above are the same. As proved above, the support count of s' as a subset of s will always be greater than or equal to the support count of s. So the denominator in the first equation above will always be greater than or equal to the denominator in the second equation, and the value of the first equation will always be less than or equal to the value of the second equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) A partitioning variation of Apriori subdivides the transactions of a database D into n nonoverlapping partitions. Prove that any itemset that is frequent in D must be frequent in at least one partition of D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a given itemset is frequent in D, then it must be locally frequent in at least one nonoverlapping partition of D. The itemset is frequent in D if:\n",
    "$$ \\frac{supportCount(I)}{totalTransactionsIn(D)} \\geq minSupport $$  \n",
    "\n",
    "If the database is divided up, at least one of the partitions will have to meet the minimum support for that local partition because if no partition did, the global support count needed would not be reached. For example, if there are 100 transactions, and the min_support is 60%, then there must be 60 instances of the itemset in the database. If this was partitioned into 5 sets of 20 transactions, then the min_support count would be 12 for each partition. If C1...C5 < 12, then the total support count would be < 60, and the itemset would not be frequent. Therefore, at least one of the partitions needs to be locally frequent if the itemset is frequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.4] Let c be a candidate itemset in Ck generated by the Apriori algorithm. How many length-(k − 1) subsets do we need to check in the prune step? Per your previous answer, can you give an improved version of procedure has infrequent subset in Figure 6.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every candidate C(k) generated, we need to check k choose k-1 subsets, which is equal to k subsets.  \n",
    "\n",
    "$$ {k \\choose k-1} = k$$  \n",
    "\n",
    "For example, if C(k) = {I1, I2, I3}, we need to check 3 choose 2, or 3 itemsets: {I1, I2}, {I1, I3}, {I2, I3}. One way to improve the procedure ```has_infrequent_subset()``` is to assume that the items which constructed the candidate set are frequent, so we don't need to check them.  So for example, (k-1) frequent itemsets ABC + ABD = ABCD (Candidate of k length). Pruning combinations to check are ABC, ABD, BCD, ACD but it was constructed from ABC, ABD, so those are assumed frequent, you only have to check BCD and ACD or (k-2) sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.5] Section 6.2.2 describes a method for generating association rules from frequent itemsets. Propose a more efficient method. Explain why it is more efficient than the one proposed there. (Hint: Consider incorporating the properties of Exercises 6.3(b), (c) into your design.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section proposes the following method for generating association rules from frequent itemsets: \n",
    "- For each frequent itemset l, generate all nonempty subsets of l.\n",
    "- For every nonempty subset s of l, output the rule “s -> (l − s)” if support count(l) ≥ min conf, where min conf is the minimum confidence threshold.\n",
    "\n",
    "While in general, confidence does not have an anti-monotone property\n",
    "c(ABC->D) can be larger or smaller than c(AB->D); confidence of rules generated from the same itemset do have an anti-monotone property. So the example provided in the text: frequent itemset X = {I1, I2, I5}, the resulting association rules:\n",
    "\n",
    "| Rule generated | Confidence |\n",
    "| :--- | ---- |\n",
    "|{I1, I2} -> I5 | confidence = 2/4 = 50% |\n",
    "|{I1, I5} -> I2 | confidence = 2/2 = 100% |\n",
    "|{I2, I5} -> I1 | confidence = 2/2 = 100% |\n",
    "|I1 -> {I2, I5} | confidence = 2/6 = 33% |\n",
    "|I2 -> {I1, I5} | confidence = 2/7 = 29% |\n",
    "|I5 -> {I1, I2} | confidence = 2/2 = 100% |\n",
    "\n",
    "The min confidence is 70%. The first rule generated {I1, I2} -> I5 doesn't meet that threshold. Since confidence is anti-monotone with respect to the right side of the rule, we know that any of the rules containing I5 on the right side will not meet the min confidence threshold either. So we can prune the two rules that contain I5 on the right: I1 -> {I2, I5} and I2 -> {I1, I5} as a result of the anti-monotone property without any calculations.\n",
    "\n",
    "On the left hand side, the proof in exercise 6.3(c) shows that for a frequent itemset I, the confidence of the rule “s′ ->(l−s′)” cannot be more than the confidence of “s->(l−s),” where s′ is a subset of s. Once again, the first rule generated {I1, I2 -> I5} doesn't meet that threshold. Rules with subsets of {I1, I2} on the left hand side can also be pruned. We prune the two rules that contain I1 and/or I2 on the left: I1 -> {I2, I5} and I2 -> {I1, I5} utilizing this enhancement as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.6] A database has five transactions. Let min sup = 60% and min conf = 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Find all frequent itemsets using Apriori and FP-growth, respectively. Compare the efficiency of the two mining processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| TID | item IDs |\n",
    "| --- | ----------------|\n",
    "|T100: | ['M','O','N','K','E','Y'] |\n",
    "|T200: | ['D','O','N','K','E','Y'] |\n",
    "|T300: | ['M','A','K','E'] |\n",
    "|T400: | ['M','U','C','K','Y'] |\n",
    "|T500: | ['C','O','O','K','I','E'] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori (frequent itemsets shown in bold):\n",
    "- Count C1:   \n",
    "{'M': 3, 'O': 3, 'N': 2, 'K': 5, 'E': 4, 'Y': 3, 'D': 1, 'A': 1, 'U': 1, 'C': 2, 'I': 1}  \n",
    "\n",
    "\n",
    "- Trim infrequent items from C1 to obtain L1:  \n",
    "**L1: {'M': 3, 'O': 3, 'K': 5, 'E': 4, 'Y': 3}**  \n",
    "  \n",
    "  \n",
    "- Generate C2:  \n",
    "[('E', 'K'), ('E', 'M'), ('E', 'O'), ('E', 'Y'), ('K', 'M'), ('K', 'O'), ('K', 'Y'), ('M', 'O'), ('M', 'Y'), ('O', 'Y')]  \n",
    "  \n",
    "\n",
    "- Count C2:  \n",
    "{('E', 'K'): 4, ('E', 'M'): 2, ('E', 'O'): 3, ('E', 'Y'): 2, ('K', 'M'): 3, ('K', 'O'): 3, ('K', 'Y'): 3, ('M', 'O'): 1, ('M', 'Y'): 2, ('O', 'Y'): 2}  \n",
    "  \n",
    "  \n",
    "- Trim infrequent items from C2 to obtain L2:  \n",
    "**L2: {('E', 'K'): 4, ('E', 'O'): 3, ('K', 'M'): 3, ('K', 'O'): 3, ('K', 'Y'): 3}**  \n",
    "  \n",
    "  \n",
    "- Generate C3:  \n",
    "[('E', 'K', 'O'), ('K', 'M', 'O'), ('K', 'M', 'Y'), ('K', 'O', 'Y')]\n",
    "  \n",
    "  \n",
    "- Prune C3 based on L2 (infrequent subsets):  \n",
    "[('E', 'K', 'O')]\n",
    "  \n",
    "  \n",
    "- Count C3:  \n",
    "{('E', 'K', 'O'): 3}  \n",
    "\n",
    "  \n",
    "- Trim infrequent to obtain L3:  \n",
    "**L3: {('E', 'K', 'O'): 3}**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPGrowth:  \n",
    "  \n",
    "- Count C1:   \n",
    "{'M': 3, 'O': 3, 'N': 2, 'K': 5, 'E': 4, 'Y': 3, 'D': 1, 'A': 1, 'U': 1, 'C': 2, 'I': 1}  \n",
    "\n",
    "\n",
    "- Trim infrequent items from C1 to obtain L1 and sort:  \n",
    "**L1: {'K': 5, 'E': 4, 'M': 3, 'O': 3, 'Y': 3}**  \n",
    "    \n",
    "    \n",
    "- Sort transactions in L order and build FPTree:  \n",
    "\n",
    "                        ROOT\n",
    "                          |\n",
    "                        K : 5\n",
    "                       /     \\\n",
    "                    E : 4   M : 1\n",
    "                   /     \\     \\\n",
    "                M : 2   O : 2  Y : 1 \n",
    "                /          \\\n",
    "            O : 1         Y : 1\n",
    "             /\n",
    "          Y : 1 \n",
    "          \n",
    "- Construct conditional pattern base from the tree\n",
    "- From this build a conditional FPTree for each item, eliminating non-frequent paths.\n",
    "- From this sub tree, generate frequent patterns.\n",
    "\n",
    "| Item | Conditional Pattern Base | Conditional FPtree | Frequent Patterns Generated\n",
    "| ---- | :----------------------------- | ------------------ | --------------------- |\n",
    "| Y | {K,E,M,O}: 1, {K,E,O}: 1, {K,M}: 1 | {K: 3} | {KY}: 3 |\n",
    "| O | {K, E, M}: 1, {K, E}: 2 | {K: 3, E: 3} | {KO}: 3, {EO}: 3, {KEO}: 3 |\n",
    "| M | {K, E}: 2, {K}: 1 | {K: 3} | {KM}: 3 |\n",
    "| E | {K}: 4 | {K: 4} | {KE}: 3 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FPGrowth process is more efficient. Two major costs associated with the Apriori algorithm are: 1.) the need to generate a huge number of candidate sets if L1 is long and 2.) the need to scan the database repeatedly and check a large set of candidates by pattern matching. The FPGrowth algorithms finds frequent itemsets without candidate generation. It transforms the problem of finding long frequent patterns into searching for shorter ones in much smaller conditional databases recursively and then concatenating the suffix. It uses the least frequent suffix first, substantially reducing the search cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) List all the strong association rules (with support s and confidence c) matching the following metarule, where X is a variable representing customers, and item i denotes variables representing items (e.g., “A,” “B,”):  \n",
    "\n",
    "$$∀x∈transaction,buys(X,item1)∧buys(X,item2)->buys(X,item3) [s,c]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following rules are generated. The two transactions shown in bold match the metarule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Rule generated | Support | Confidence |\n",
    "| :--- | ---- | ---- |\n",
    "|{E} -> {K} | support = 4/5 = 80% | confidence = 4/4 = 100% |\n",
    "|{K} -> {E} | support = 4/5 = 100% | confidence = 4/5 = 80% |\n",
    "|{O} -> {E} | support = 3/5 = 60% | confidence = 3/3 = 100% |\n",
    "|{M} -> {K} | support = 3/5 = 60% | confidence = 3/3 = 100% |\n",
    "|{O} -> {K} | support = 3/5 = 60% | confidence = 3/3 = 100% |\n",
    "|{Y} -> {K} | support = 3/5 = 60% | confidence = 3/3 = 100% |\n",
    "|**{E,O} -> {K}** | **support = 3/5 = 60%** | **confidence = 3/3 = 100%** |\n",
    "|**{K,O} -> {E}** | **support = 3/5 = 60%** | **confidence = 3/3 = 100%** |\n",
    "|{O} -> {K, E} | support = 3/5 = 60% | confidence = 3/3 = 100% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.11] Most frequent pattern mining algorithms consider only distinct items in a transaction. However, multiple occurrences of an item in the same shopping basket, such as four cakes and three jugs of milk, can be important in transactional data analysis. How can one mine frequent itemsets efficiently considering multiple occurrences of items? Propose modifications to the well-known algorithms, such as Apriori and FP-growth, to adapt to such a situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to handle this in both the Apriori and FP-growth algorithm is to treat each instance of a duplicate item as a unique item. So rather than compressing a transaction with 4 cakes and 3 jugs of milk to {cake, milk} the transaction would be: {cake_1, cake_2, cake_3, cake_4, milk_1, milk_2, milk_3}. The benefit of this analysis would be you could analyze association rules like, if you buy 2 cartons of eggs, how likely are you to buy 2 cartons of milk? The problem with this is that it dramatically increases the computational complexity of the analysis because the number of unique items increases. Given d itemsets, the total number of itemsets = 2^d, and the total possible association rules = 3^d + 2^(d+1) + 1. So any increase in d will increase the computational complexity of the analysis exponentially. In this situation, counting using hash structure would greatly improve the efficiency of this analysis. It would also be wise to use FPGrowth over Apriori since L1 would likely increase significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
