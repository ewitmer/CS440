{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erin Witmer  \n",
    "CSC 440"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.1] Suppose you have the set C of all frequent closed itemsets on a data set D, as well as the support count for each frequen closed itemset. Describe an algorithm to determine whether a given itemset X is frequent or not, and the support of X if it is frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset X is closed in set D if there exists no superset Y such than Y has the same support count as X in D. Per the text: \"The set of closed frequent itemsets contains complete information regarding the frequent itemsets.\" A simple algorithm to determine whether X is frequent or not and the support is:\n",
    "\n",
    "``` \n",
    "sort set C by support count (high to low)\n",
    "\n",
    "for each itemset (i) in C:\n",
    "    \n",
    "    if X is a subset of C(i):\n",
    "       return support count for this item \n",
    "            \n",
    "    else:\n",
    "       next item\n",
    "    \n",
    "return X is not frequent if X is not a subset of any itemset in C \n",
    "```\n",
    "We know that the support count of X if it is a subset of the itemset in C is the same as the support count in that itemset based on the definition of the closed itemset. We know it is frequent because C is a set of all frequent closed itemsets. We know that if X is not a subset of any itemset in C, it is not frequent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.3] The Apriori algorithm makes use of prior knowledge of subset support properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Prove that all nonempty subsets of a frequent itemset must also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset (I) is a frequent itemset if the percentage of transactions (T) in a database (D) containing that itemset (I) meets or exceeds a minimum support threshold (min_sup). That is: $$ \\frac{countOfTransactionsContaining(I)}{totalTransactionsIn(D)} \\geq minSupport \\\\$$if $$ A \\subset I\n",
    "$$ then the count of transactions containing A is greater than or equal to the count of transactions containing I, because every transaction containing I, will also contain A. This is true for all non-empty subsets of I. The denominator, the count of all transactions, does not change. So if the numerator (count of transactions containing A) is greater than or equal to the count of transactions containing I, and the denominator is constant, then the support level will be greater than or equal to the support level of I. If I is frequent, or higher than the min support level, then A will also be frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Prove that the support of any nonempty subset s′ of itemset s must be at least as great as the support of s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support count of s is the number of times an itemset (s) appears in the universe of all transactions. For example, if s = {beer, nuts, diapers} then the support count of s is the number of transactions which contain all three of those items. Any subset of s, for example, s' = {beer, nuts} will occur in all transactions in which s occurs. In other words, any transaction which contains {beer, nuts, diapers} will also contain all subsets of that itemset {beer, nuts}. The support count of s' will therefore be at least as high as s, and may be higher if the subset s' occurs in additional transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Given frequent itemset l and subset s of l, prove that the confidence of the rule “s′ ⇒(l−s′)” cannot be more than the confidence of “s⇒(l−s),” where s′ is a subset of s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ confidence(s'⇒l-s')= \\frac{supportCount(s' ∪ (l-s'))}{supportCount(s')}$$\n",
    "\n",
    "$$confidence(s⇒l-s)= \\frac{supportCount(s ∪ (l-s))}{supportCount(s)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know s' is a subset of s and therefore a subset of l, l-s' represents the complement to the frequent itemset, l. In union with s', it equals the frequent itemset. So the count of s' ∪ (l-s') will be the support count of l. Similarly, l-s represents the complement to the frequent itemset, and the count of s ∪ (l-s) will be the count of l. So we know the numerators in the equations above are the same. As proved above, the support count of s' as a subset of s will always be greater than or equal to the support count of s. So the denominator in the first equation above will always be greater than or equal to the denominator in the second equation, and the value of the first equation will always be less than or equal to the value of the second equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) A partitioning variation of Apriori subdivides the transactions of a database D into n nonoverlapping partitions. Prove that any itemset that is frequent in D must be frequent in at least one partition of D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a given itemset is frequent in D, then it must be locally frequent in at least one nonoverlapping partition of D. The itemset is frequent in D if:\n",
    "$$ \\frac{supportCount(I)}{totalTransactionsIn(D)} \\geq minSupport $$\n",
    "If the database is divided up, at least one of the partitions will have to meet the minimum support for that local partition because if no partition did, the global support count needed would not be reached. For example, if there are 100 transactions, and the min_support is 60%, then there must be 60 instances of the itemset in the database. If this was partitioned into 5 sets of 20 transactions, then the min_support count would be 12 for each partition. If C1...C5 < 12, then the total support count would be < 60, and the itemset would not be frequent. Therefore, at least one of the partitions needs to be locally frequent if the itemset is frequent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.4] Let c be a candidate itemset in Ck generated by the Apriori algorithm. How many length-(k − 1) subsets do we need to check in the prune step? Per your previous answer, can you give an improved version of procedure has infrequent subset in Figure 6.4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every candidate C(k) generated, we need to check k choose k-1 subsets, which is equal to k subsets. \n",
    "$$ {k \\choose k-1} = k$$\n",
    "For example, if C(k) = {I1, I2, I3}, we need to check 3 choose 2, or 3 itemsets: {I1, I2}, {I1, I3}, {I2, I3}. One way to improve the procedure ```has_infrequent_subset()``` is to assume that the items which constructed the candidate set are frequent, so we don't need to check them.  So for example, (k-1) frequent itemsets ABC + ABD = ABCD (Candidate of k length). Pruning combinations to check are ABC, ABD, BCD, ACD but it was constructed from ABC, ABD, so those are assumed frequent, you only have to check BCD and ACD or (k-2) sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.5] Section 6.2.2 describes a method for generating association rules from frequent itemsets. Propose a more efficient method. Explain why it is more efficient than the one proposed there. (Hint: Consider incorporating the properties of Exercises 6.3(b), (c) into your design.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section proposes the following method for generating association rules from frequent itemsets: \n",
    "- For each frequent itemset l, generate all nonempty subsets of l.\n",
    "- For every nonempty subset s of l, output the rule “s ⇒ (l − s)” if support count(l) ≥ min conf, where min conf is the minimum confidence threshold.\n",
    "\n",
    "While in general, confidence does not have an anti-monotone property\n",
    "c(ABC →D) can be larger or smaller than c(AB →D); confidence of rules generated from the same itemset do have an anti-monotone property. So the example provided in the text: frequent itemset X = {I1, I2, I5}, the resulting association rules:\n",
    "\n",
    "| Rule generated | Confidence |\n",
    "| :--- | ---- |\n",
    "|{I1, I2} ⇒ I5 | confidence = 2/4 = 50% |\n",
    "|{I1, I5} ⇒ I2 | confidence = 2/2 = 100% |\n",
    "|{I2, I5} ⇒ I1 | confidence = 2/2 = 100% |\n",
    "|I1 ⇒ {I2, I5} | confidence = 2/6 = 33% |\n",
    "|I2 ⇒ {I1, I5} | confidence = 2/7 = 29% |\n",
    "|I5 ⇒ {I1, I2} | confidence = 2/2 = 100% |\n",
    "\n",
    "The min confidence is 70%. The first rule generated {I1, I2} ⇒ I5 doesn't meet that threshold. Since confidence is anti-monotone with respect to the right side of the rule, we know that any of the rules containing I5 on the right side will not meet the min confidence threshold either. So we didn't need to check the two rules that contain I5 on the right: I1 ⇒ {I2, I5} and I2 ⇒ {I1, I5} as a result of the anti-monotone property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6.11] Most frequent pattern mining algorithms consider only distinct items in a transaction. However, multiple occurrences of an item in the same shopping basket, such as four cakes and three jugs of milk, can be important in transactional data analysis. How can one mine frequent itemsets efficiently considering multiple occurrences of items? Propose modifications to the well-known algorithms, such as Apriori and FP-growth, to adapt to such a situation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A simple way to handle this in both the Apriori and FP-growth algorithm is to treat each instance of a duplicate item as a unique item. So rather than compressing a transaction with 4 cakes and 3 jugs of milk to {cake, milk} the transaction would be: {cake_1, cake_2, cake_3, cake_4, milk_1, milk_2, milk_3}. The benefit of this analysis would be you could analyze association rules like, if you buy 2 cartons of eggs, how likely are you to buy 2 cartons of milk? The problem with this is that it dramatically increases the computational complexity of the analysis because the number of unique items increases. Given d itemsets, the total number of itemsets = 2^d, and the total possible association rules = 3^d + 2^(d+1) + 1. So any increase in d will increase the computational complexity of the analysis exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 3, 'Y': 3, 'M': 3, 'K': 5, 'E': 4, ('E', 'K'): 4, ('E', 'O'): 3, ('K', 'M'): 3, ('K', 'O'): 3, ('K', 'Y'): 3, ('E', 'K', 'O'): 3}\n"
     ]
    }
   ],
   "source": [
    "test = {'T100':['M','O','N','K','E','Y'],\n",
    "        'T200':['D','O','N','K','E','Y'],\n",
    "        'T300':['M','A','K','E'],\n",
    "        'T400':['M','U','C','K','Y'], \n",
    "        'T500':['C','O','O','K','I','E']}\n",
    "\n",
    "all_freq = find_frequent_itemsets(test, .6)\n",
    "print(all_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemset_difference(itemset, subset):\n",
    "    \"\"\"Generates a list of two tuples: [(s), (l-s)].\n",
    "    Args:\n",
    "      itemset: a frequent itemset.\n",
    "      subset: a subset of the frequent itemset.\n",
    "    Raises:\n",
    "      TypeError: If itemset is not of type tuple.\n",
    "      TypeError: If subset is not of type tuple.\n",
    "    Returns:\n",
    "      The list: [s, (l-s)].\n",
    "    \"\"\"    \n",
    "    if (type(itemset)) != tuple:\n",
    "        raise TypeError('itemset needs to be a tuple')\n",
    "    if (type(subset)) != tuple:\n",
    "        raise TypeError('subset needs to be a tuple')        \n",
    "        \n",
    "    diff = [tuple(sorted(subset)), tuple(sorted(set(itemset).difference(set(subset))))]\n",
    "    \n",
    "    if len(diff[0]) == 1:\n",
    "        diff[0] = ''.join(diff[0])\n",
    "\n",
    "    if len(diff[1]) == 1:\n",
    "        diff[1] = ''.join(diff[1])        \n",
    "    \n",
    "    return diff\n",
    "    \n",
    "def generate_candidate_rules(freq_itemset):\n",
    "    \"\"\"Generates a list of all rules candidates for an itemset: [(s), (l-s)].\n",
    "    Args:\n",
    "      freq_itemset: a frequent itemset in the form of a tuple.\n",
    "    Returns:\n",
    "      All possible rules for the itemset.\n",
    "    \"\"\"\n",
    "    k = len(freq_itemset) \n",
    "    all_rules = []\n",
    "    \n",
    "    for i in range(k-1,0,-1):  # loop through k choose i\n",
    "        combinations = list(itertools.combinations(freq_itemset, i))  # get all combinations\n",
    "        candidates = list(map(lambda x: itemset_difference(freq_itemset, x), combinations))\n",
    "        all_rules.extend(candidates)\n",
    "    \n",
    "    return all_rules\n",
    "\n",
    "def get_support_confidence(rule_set, support_count, length, all_frequent):\n",
    "    support = support_count / length\n",
    "    confidence = support_count / all_frequent[rule_set[0]]\n",
    "    \n",
    "    rule_set.extend([support, confidence])\n",
    "    return rule_set\n",
    "\n",
    "def prune_candidate_rules(freq_itemset, all_frequent, min_confidence, length):\n",
    "    \"\"\"Tests the candidates for one frequent itemset and prunes < min_confidence\n",
    "    Args:\n",
    "      freq_itemset: a frequent itemset in the form of a tuple.\n",
    "      all_frequent: a dict of all frequent itemsets\n",
    "      min_confidence: minimum confidence level\n",
    "    Raises:\n",
    "      TypeError: If all_frequent is not a dictionary.\n",
    "      ValueError: If freq_itemset is not of type tuple.\n",
    "    Returns:\n",
    "      All possible rules for the itemset.\n",
    "    \"\"\"    \n",
    "    \n",
    "    if (type(all_frequent)) != dict:\n",
    "        raise TypeError('freq_itemset needs to be in dictionary format')\n",
    "        \n",
    "    if min_confidence > 1 or min_confidence < 0:\n",
    "        raise ValueError('support_pct must be in the range [0,1]') \n",
    "        \n",
    "    support_count = all_frequent[freq_itemset]  # numerator is same for all rules\n",
    "    max_count = math.floor(support_count / min_confidence)  # max denominator\n",
    "    all_rules = generate_candidate_rules(freq_itemset)\n",
    "    pruned_rules = list(filter(lambda x: all_frequent[(x[0])] <= max_count, all_rules))\n",
    "    list(map(lambda x: get_support_confidence(x, support_count, length, all_frequent), pruned_rules))\n",
    "    \n",
    "    return pruned_rules\n",
    "\n",
    "    \n",
    "def generate_all_rules(all_frequent, min_confidence, length):\n",
    "    \n",
    "    associations = [] \n",
    "    for key, value in all_frequent.items():\n",
    "        pruned_rules = prune_candidate_rules(key, all_frequent, min_confidence, length)\n",
    "        associations.extend(pruned_rules)\n",
    "        \n",
    "    return associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['E', 'K', 5.0, 1.0],\n",
       " ['K', 'E', 5.0, 0.8],\n",
       " ['E', 'O', 3.75, 0.75],\n",
       " ['O', 'E', 3.75, 1.0],\n",
       " ['K', 'M', 3.75, 0.6],\n",
       " ['M', 'K', 3.75, 1.0],\n",
       " ['K', 'O', 3.75, 0.6],\n",
       " ['O', 'K', 3.75, 1.0],\n",
       " ['K', 'Y', 3.75, 0.6],\n",
       " ['Y', 'K', 3.75, 1.0],\n",
       " [('E', 'K'), 'O', 3.75, 0.75],\n",
       " [('E', 'O'), 'K', 3.75, 1.0],\n",
       " [('K', 'O'), 'E', 3.75, 1.0],\n",
       " ['E', ('K', 'O'), 3.75, 0.75],\n",
       " ['K', ('E', 'O'), 3.75, 0.6],\n",
       " ['O', ('E', 'K'), 3.75, 1.0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_all_rules(all_freq, .6, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
