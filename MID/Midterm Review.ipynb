{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why data mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The proliferation of devices collecting data, decreased cost to store data, and increased speed and ability to process data has increased the opportunity to turn large sets of data into knowledge with commercial and societal benefits.  \n",
    "  \n",
    "- Example: Turning Google's search data on flu symptoms into flu trends faster than traditional reporting systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is data mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data mining the process of discovering interesting patterns from massive amounts of data. It is extracting non-trivial, previously unknown knowledge from large quantities of data by automated or semi-automated means.\n",
    "- The process is:\n",
    "    - *preprocessing*: feature selection, dimensionality reduction, normalization, data subsetting\n",
    "    - *mining*: methods applied to extract patterns\n",
    "    - *postprocessing*: filtering patterns, visualization, interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What kind of data can be mined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While nearly any type of data can be mined, relational and transactional data in databases are common sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What kind of patterns can be mined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data mining tasks can be classified as:\n",
    "    - *descriptive*: find human interpretable patterns that describe the data.\n",
    "    - *predictive*: Use variables to predict unknown future values of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Descriptive tasks (descriptive, supervised):\n",
    "    - *data characterization*: summarizing the data of the class in general terms. For example, summarize the general characteristics of customers who spend more than $5,000 / yr \"big spenders\". The general profile may include income, credit, age, etc.\n",
    "    - *data discrimination*: similar to data characterization, but comparing two classes. For example, how does the general profile of a \"frequent shopper\" differ from an \"infrequent shopper\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequent patterns, associations and correlations (predictive, supervised):\n",
    "    - *frequent patterns*: patterns that occur frequently in the data\n",
    "    - *association analysis*: a predictive attribute such as 'buy' that repeats in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification and regression (predictive, supervised):\n",
    "    - *classification*: the process of finding a model that distinguishes data classes or concepts. The process relies on labeled data. Decision trees, neural networks, naive Bayes, k-nearest neighbor are common classification methods. Example: model for predicting creditworthiness (decision tree), classifying a breed of dog (neural network).\n",
    "    - *regression*: similar to classification, but models continuous valued functions, used to predict missing numerical value. Example: predict sales based on ad spend, stock price, wind speed, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster Analysis (descriptive, unsupervised):\n",
    "    - *cluster analysis*: finding groups of objects that minimize intra-cluster distance and maximize inter-cluster distance. Does not consult labels. Examples include market segmentation and text similarity analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outlier analysis:\n",
    "    - Data that does not comply with the model. In some situations, such as fraud detection, network invasion, deforestation: anomaly mining is more interesting than the regular occurrences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not all patterns are interesting. A pattern is interesting if it is: easily understood, valid on new data, useful and new. Association analysis examples that may be interesting: market basket analysis, alarm diagnosis, medical diagnosis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which technologies are used? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data mining has incorporated many techniques from other domains such as stats, ML, database systems.  Data mining is closely related to all of these disciplines. \n",
    "- It is not uniquely a transformation or application of one of these disciplines but rather an evolution of all of them in response to the need for “effective, scalable, and flexible data analysis in our society”. \n",
    "- As data sets become massive and diverse, data mining techniques must be computationally efficient, handle various data types and produce effective results. This is distinct from the more constrained and controlled worlds of traditional disciplines such as statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major issues in data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main challenge presented when mining a huge amount of data is the efficiency and scalability of the data mining algorithm used to store and process the data effectively.  Finding an algorithm that can process the data effectively, efficiently and increasingly in real-time.\n",
    "- Another challenge is dealing with the increasing diversity of data types. Data is no longer of a uniform data type and stored neatly in a relational database. It is diverse, dynamic and distributed. One way to deal with these challenges is to utilize parallel and distributed data-intensive mining algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2: Getting to Know Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data objects and attribute types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data is a collection of objects and their attributes.\n",
    "- A data attribute is a property or characteristic of an object (eye color, income).\n",
    "- A collection of attributes describe an object. \n",
    "- Attribute values can be nominal, binary, ordinal or numeric.\n",
    "    - *nominal*: categorical, the values have no meaningful order. \n",
    "    - *binary*: two possible outcomes. \n",
    "        - symmetric: equally likely/valuable such as gender\n",
    "        - asymmetric: one value, such as a positive medical test, is more valuable\n",
    "    - *ordinal*: categorical, the values have a rank order.\n",
    "    - *numeric*: quantitative, measurable value.\n",
    "        - interval-scaled: relative, no true zero exists (temp, year)\n",
    "        - ratio-scaled: absolute, true zero exists (temp in K, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The type of an attribute depends on the properties/operations it possesses:\n",
    "    - Nominal attribute: distinctness (=, !=)\n",
    "    - Ordinal attribute: distinctness & order (<, >)\n",
    "    - Interval attribute: distinctness, order & meaningful differences (+, -)\n",
    "    - Ratio attribute: distinctness, order & meaningful differences and ratios (*, /)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discrete vs. continuous variables:\n",
    "    - Discrete values are finite or countably infinite (age 0-110)\n",
    "    - Continuous values have real numbers as attributes, typically represented by floating point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Important characteristics of data include: \n",
    "    - *Dimensionality*: number of attributes\n",
    "    - *Sparsity*: Only presence counts\n",
    "    - *Resolution*: Patterns depend on the scale\n",
    "    - *Size*: Type of analysis may depend on size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Types of data sets:\n",
    "    - *Record*: A collection of records, each with a fixed set of attributes. This can be represented by a matrix or a vector. Transaction data is a special type of record data.\n",
    "    - *Graph*: Represented by edges and vertices, and may represent things like molecular structures or websites.\n",
    "    - *Ordered*: May be a sequence of transactions, temporal data, genomic sequence, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Noise and outliers: For objects, extraneous objects; for attributes: modification of original values. Outliers are objects with considerably different characteristics than the rest of the data set.\n",
    "- Missing values: Information is either not collected or not relevant in all cases.\n",
    "- Duplicate data: same person with multiple entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic statistical descriptions of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measures of central tendency\n",
    "    - *mean*: average\n",
    "    - *median*: middle\n",
    "    - *mode*: most common\n",
    "    - *midrange*: average of max/min\n",
    "    - *negative skew*: long left tail\n",
    "    - *positive skew*: long right tail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measures of dispersion of data\n",
    "    - *quartiles*: Q1(25th), Q3(75th)\n",
    "    - *interquartile range*: Q3 - Q1\n",
    "    - *five number summary*: min, Q1, median, Q3, max\n",
    "    - *boxplot*: outliers marked (>1.5x IQR), whiskers are min/max, quartiles are marked\n",
    "    - *variance*: sum(x^2)/n - mean^2\n",
    "    - *standard deviation*: sqrt(variance), measures the spread around the mean\n",
    "        - mean +/- 1 sigma: 68% under normal curve\n",
    "        - mean +/- 2 sigma: 95% under normal curve\n",
    "        - mean +/- 3 sigma: 99.7% under normal curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphic displays of basic statistical descriptions of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Univariate:\n",
    "    - Quantile plot: Sort in ascending order, calculate the fraction of each data point as: f = (i-0.5)/N. Plot x = f-value, y = data\n",
    "    - Quantile-quantile plot: Calculate the quantile of each, plot the data at each f-value against each other.\n",
    "    - Histogram: plot the data distribution in bins\n",
    "    \n",
    "- Bivariate: \n",
    "    - Scatter plot: treat each pair as (x, y)\n",
    "    - Correlation: positive, negative or no correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity and dissimilarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dissimilarity measures:\n",
    "    - *nominal*: [0,1] are 0 for similar objects, 1 for dissimilar objects. \n",
    "    - *ordinal*: d = |x-y|/(n-1), values are mapped to integers 0 to n-1.\n",
    "    - *interval or ratio*: d = |x-y|\n",
    "- Similarity measures: \n",
    "    - *nominal*: [0,1] are 1 for similar objects, 0 for dissimilar objects.\n",
    "    - *ordinal*: s = 1-d\n",
    "    - *interval or ratio*: s = -d, s = 1/(1+d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Euclidean distance = $ \\sqrt{\\sum_{i=0}^n (x_{k}-y_{k})^2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minkowski distance = $ ({\\sum_{i=0}^n (x_{k}-y_{k})^r})^\\frac{1}{r} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n is the number of dimensions (attributes), r is the dimension.\n",
    "- r = 1: Manhattan distance\n",
    "- r = 2: Euclidean distance\n",
    "- r = infinity: max distance between any attribute max(|x1 - x2|,|y1 - y2|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (?) Mahalanobis distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Properties of distance:\n",
    "    - d(x,y) >= 0 (Positive definiteness)\n",
    "    - d(x,y) = d(y,x) (symmetry)\n",
    "    - d(x,z) <= d(x,y) + d(y,z) (Triangle inequality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Properties of similarity:\n",
    "    - max similarity = 1 if x = y\n",
    "    - s(x,y) = s(y,x) (symmetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity for binary attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute similarities using the following quantities\n",
    "    - f01 = the number of attributes where p was 0 and q was 1\n",
    "    - f10 = the number of attributes where p was 1 and q was 0\n",
    "    - f00 = the number of attributes where p was 0 and q was 0\n",
    "    - f11 = the number of attributes where p was 1 and q was 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple matching = number of matches / number of attributes\n",
    "    - (f11 + f00) / (f01 + f10 + f11 + f00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jaccard = number of 11 matches / number of non-zero attributes\n",
    "    - (f11) / (f01 + f10 + f11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good for long sparse data sets the similarity of two documents. Measures the cosine of the angle between two vectors, determines whether they are pointing in roughly the same direction. \n",
    "    - cos(d1, d2) = d1 (dot) d2 / ||d1|| ||d2||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extended Jaccard Coefficient (Tanimoto)\n",
    "    - (x dot y)/(||x||^2+||y||^2 - x dot y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correlation measures the linear relationship between objects\n",
    "    - cov(x,y) = (1/n-1) x sum(x deviation * y deviation)\n",
    "    - corr = cov/(sx x sy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Based Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The more certain an outcome, the less information that it contains and vice-versa\n",
    "    - If a coin has 2 heads, a flip provides no additional information.\n",
    "    - Information is inversely related to the probability of an outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is commonly measured as entropy: -$ \\sum_{i=1}^n p_{i}log_{2}p_{i}$\n",
    "- Entropy is between 0 and $log_{2}n$\n",
    "- Joint entropy: H(X) + H(Y) - H(x,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (?) General approach for combining similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures of density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Euclidean Density: grid based approach\n",
    "    - Divide region into cells of equal volume, count the points in each cell\n",
    "- Euclidean Density: center-based approach\n",
    "    - The number of points within a specified radius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data quality depends on: accuracy, completeness, consistency, timeliness, believability, interpretability. \n",
    "- The major tasks in data preprocessing include:\n",
    "    - Data cleaning: fill missing values, smooth noisy data, remove ouliers, resolve inconsistency.\n",
    "    - Data integration: combining various data sets / sources\n",
    "    - Data reduction: Compressing data, removing irrelevant attributes\n",
    "    - Numerosity reduction: replace data with smaller representation. \n",
    "    - Data transformation: normalization, data discretization, concept hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Missing values\n",
    "    - Ignore it, fill it in manually, use a global constant to fill it, use a central tendency for the attribute, use a central tendency \n",
    "- Noisy data\n",
    "    - Smoothing by bin means: each value replaced by the mean of the bin\n",
    "    - Smoothing by bin medians, smoothing by bin boundaries: each value is replaced by max or min of the bin.\n",
    "    - Regression: find best line to fit the data\n",
    "    - Outlier analysis: use cluster analysis \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merging of various data sources. \n",
    "    - Entity identification problem (matching up sources)\n",
    "    - Redundancy - can be detected by correlation analysis (chi-square or correlation coefficient)\n",
    "    - (?) chi-square calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aggregation: combining two or more attributes into a single attribute. \n",
    "    - Smaller in data, but maintains most of the integrity of the original data\n",
    "    - Change of scale from weeks to years, cities to states, etc. (concept hierarchy)\n",
    "    - Purpose: data reduction, change of scale, more stable data\n",
    "- Histograms:\n",
    "    - equal width vs. equal frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Processing the entire data set is too expensive or time consuming\n",
    "    - sample must be representative \n",
    "    - simple random sampling w/ or w/out replacement\n",
    "    - cluster sample\n",
    "    - stratified sampling: ensures a representative sample when data is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When dimensionality increases, data becomes sparse and definitions of density become less meaningful \n",
    "- Purpose of reducing dimensionality: avoid curse of dimensionality, reduce time and memory to mine, allow data to be visualized.\n",
    "- Techniques: \n",
    "    - Principal components analysis (PCA): find a projection that captures the largest amount of variation in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature subset selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove redundant features, irrelevant features, especially for classification. \n",
    "- Trim based on information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create new attributes that can capture the important information in a data set more efficiently than the original methodologies:\n",
    "    - feature extraction, feature construction, mapping data to new space\n",
    "    - (?) Fourier and wavelet transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The process of converting a continuous attribute into an ordinal one\n",
    "    - Commonly used in classification, most work best with only a few possible variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maps continuous or categorical values to binary variables,\n",
    "    - Typically used for association analysis (asymmetric binary attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function maps the entire set of values to new values\n",
    "    - log(x)\n",
    "    - normalization, standardization\n",
    "    - example: seasonality accounts for much of correlation of plant growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Max/min nomalization: (v - min) / (max - min) * (new_max - new_min) + new_min\n",
    "    - range likely [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Z-score normalization (v - avg)/sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization by decimal scaling: v = v / 10^j range [0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept hierarchy for nominal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- May be generated based on the number of distinct values per attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequent pattern: a pattern (set of items, subsequences, substructures) that occur frequently in a data set.\n",
    "    - What products are purchased together? (basket data analysis)\n",
    "    - What products are purchased after purchasing a product? (cross marketing)\n",
    "    - Web log analysis, DNA sequence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Market basket analysis:\n",
    "    - $support(A \\rightarrow B) = P(A \\cup B) = count(A \\cup B) / n$\n",
    "    - $confidence(A \\rightarrow B) = P(A | B) = count(A \\cup B) / count(A)$\n",
    "    - Find all frequent itemsets\n",
    "    - Generate all strong association rules from each frequent itemset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Closed and max sets\n",
    "    - An itemset X is closed if X is frequent and there exists no super-pattern Y $\\supset$ X with the same support as X\n",
    "        - Y must have the same support as X, this is not a lossy compression and contains complete information\n",
    "    - An itemset X is maximal if X is frequent and there exists no super-pattern Y $\\supset$ X and Y is frequent\n",
    "        - Y only has to be frequent, this is a lossy compression, doesn't contain complete information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori: Finding Frequent Itemsets by Confined Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downward closure property of frequent patterns: any subset of a frequent itemset must be frequent. \n",
    "- Scan the database to get all frequent 1-itemsets\n",
    "- Generate candidates of length (k+1) from L1. \n",
    "- Prune the itemsets that are not frequent.\n",
    "- Generate candidates of length (k+1) if (k-1) = (k-1).\n",
    "- Look at all (k-1) subsets of the candidate and prune any that are not frequent\n",
    "- Scan for count of itemset lenght k and repeat until empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problems with Apriori: multiple scans of the database, many candidates, support counting\n",
    "    - breadth first, huge number of candidates\n",
    "- (?) Improvements to the algorithm: Candidate itemsets stored in a hash tree.\n",
    "    - Others include: Transaction reduction, partitioning, sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern-Growth Approach: Mining Frequent Patterns Without Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depth first search \n",
    "- Avoids explicit candidate generation\n",
    "- Grow long patterns from short ones using locally frequent items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct an FP Tree \n",
    "    - Find frequent 1 itemset \n",
    "    - order it\n",
    "    - scan each transaction and create a tree based on the sorted list.\n",
    "- Create conditional pattern bases from the paths up the tree\n",
    "    - For each pattern base, create a conditional FP Tree\n",
    "    - From the conditional FP Tree, generate frequent patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Benefits of the FP Tree structure\n",
    "    - Completeness, compactness, \n",
    "    - Divide-and-conquer: \n",
    "        - Decompose both the mining task and DB according to the frequent patterns obtained so far\n",
    "         - Lead to focused search of smaller databases\n",
    "         - No candidate generation, no candidate test\n",
    "         - Compressed database: FP-tree structure \n",
    "         - No repeated scan of entire database "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which patterns are interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes, association does not measure the real strength (or lack of strength) of the correlation and implication between A and B. They don't filter out uninteresting association rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lift: $P(A \\cup B) / (P(A) * P(B))$   >1 = positive correlation;  <1 = negative\n",
    "- Chi-square "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Four other measures defined have the following property: Its value is only influenced by the supports of A, B, and A ∪ B, or more exactly, by the conditional probabilities of P(A|B) and P(B|A), but not by the total number of transactions. \n",
    "- Another common property is that each measure ranges from 0 to 1, and the higher the value, the closer the relationship between A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A measure is null-invariant if its value is free from the influence of null-transactions. Null-invariance is an important property for measuring association patterns in large transaction databases. Among the six discussed measures in this subsection, only lift and χ2 are not null-invariant measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning: training data with labels indicating the class of the observation (classification)\n",
    "- Unsupervised learning: the class labels of training data is not known, looking to establish classes (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction problems:\n",
    "    - Classification: predicts category\n",
    "        - Typical applications: loan approval, medical diagnosis, fraud detection, web categorization\n",
    "    - Numeric prediction: regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification is a two-step process. \n",
    "    - Model construction: use the samples/tuples as the training set\n",
    "        - Model represents classification rules, decision trees, mathematical formula\n",
    "    - Model usage: used to classify unknown objects\n",
    "        - estimate accuracy: rate of test samples that are correctly classified. Test set is independent to avoid overfitting.\n",
    "        - if the accuracy is acceptable, it is used to classify new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree induction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flowchart like structure where every node represents a test of an attribute and each node represents an outcome. \n",
    "- Decision trees are popular because they don't require any domain knowledge or parameter setting so they are appropriate for exploration. They are simple and intuitive. Frequently used in medicine and astronomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic algorithm (a greedy, non-backtracking algorithm):\n",
    "    - Tree is constructed in top-down recursive divide and conquer manner\n",
    "    - At the start, all training examples are at the root.\n",
    "    - All attributes are categorical\n",
    "    - Attributes are partitioned recursively based on selected attributes\n",
    "    - Test attributes are selected on the basis of information gain.\n",
    "    - Stop if: \n",
    "        - all samples for a node belong to the same class\n",
    "        - there are no attributes left, there are no samples left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy is a measure of the uncertainty \n",
    "- p is the probability that a tuple in D belongs to C: |C|/|D|\n",
    "- Expected information (entropy) needed to classify tuple:  -$ \\sum_{i=1}^n p_{i}log_{2}p_{i}$\n",
    "- Information needed after: weighted average entropy for each of the new partitions\n",
    "- Information gain: Info(D) - Info(D)after_partition\n",
    "- When working with continuous data, an optimum split-point must be found\n",
    "- Problem with information gain: tends to favor splits with a higher number of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (?) Gain ratio normalizes gain with split info: \n",
    "- GainRatio = Gain(A)/SplitInfo(A)\n",
    "- Problem with gain ratio: Tends to prefer unbalanced splits where one partition is much smaller than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split data into two subsets\n",
    "- gini(D) = 1 - $\\sum_{j=1}^np_j^2$\n",
    "- gini(D) = weighted average gini post split\n",
    "- need to enumerate all the possible splitting points (all possible binary subsets)\n",
    "- Problem with gini index: biased towards multivaried attributes and difficulty when number of classes is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A tree may have too many branches, may reflect anomalies due to noise or outliers\n",
    "- No one method is measurably more effective than another, they all have biases\n",
    "- Two approaches to avoid overfitting: \n",
    "    - prepruning: halt early if gain is below a threshold\n",
    "    - postpruning: remove branches from a fully grown tree\n",
    "- Problem with the decision tree is storing the large data set in memory.\n",
    "    - (?) RainForest framework: builds a list of (attribute, value, class_label)\n",
    "    - BOAT bootstraps smaller trees into a bigger tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumes class-conditional independence, which simplifies the calculation (why it's called Bayes) \n",
    "- It is a statistical classifier - predicts class membership probabilities\n",
    "- Performance is a standard of optimal decision making. \n",
    "- Incremental: each training set can increase/decrease the probability that a hypothesis is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayes Theorem: P(H|X) = [P(X|H) x P(H)] / P(X)\n",
    "- Maximum of P(C|X) for all classes\n",
    "- Practical difficulty: requires initial knowledge of many probabilities involving significant computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since P(X) is constant, only P(X|C) x P(C) needs to be maximized. If class probabilities are not known, only P(X|C) needs to be maximized.\n",
    "- P(X|C) is the product of all the individual attribute conditional probabilities\n",
    "- Each conditional probability must be non-zero, use a Laplacian correction, adding 1 to each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advantages: easy to implement, good results in most cases.\n",
    "- Disadvantages: Assumes conditional independence, dependencies likely exist but can't be modeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule-Based Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rules are easier to understand than large trees; one rule is created for each path of the tree\n",
    "- Rules are mutually exclusive and exhaustive\n",
    "- Rule antecedent/precondition vs rule consequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Coverage = (tuples covered by the rule) / (number of tuples in D)\n",
    "- Accuracy = (tuples correctly identified) / (tuples covered by the rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If more than one rule is triggered during classification, need a strategy to deal with it:\n",
    "    - size ordering: assign to the most attributes\n",
    "    - class-based ordering: decreasing order of prevalence or cost of misclassification\n",
    "    - rule-based ordering: make a priority list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential covering method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IF-THEN rules can be extracted directly from the data without building a tree using a sequential covering method.\n",
    "- Rules are learned one at a time. Each time a rule is learned the tuples covered by the rules are removed\n",
    "- Learn the best rule for the current class, use a greedy depth first search to do this\n",
    "- (?) FOIL considers/favors both coverage and accuracy\n",
    "- FOIL_Prune prunes if higher for pruned R than R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use test data set when measuring accuracy\n",
    "- Methods for estimating accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confusion matrix (prdicted on X, actual on Y):  \n",
    "    \n",
    "|  |YES  |NO  |TOTAL  |  |\n",
    "|---|---|---|---|---|\n",
    "| YES |TP  |FN  |P  |  |\n",
    "| NO |FP  |TN  |N  |  |\n",
    "|  | P' |N'  |P+N |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: % of tuples that are correctly classified = (TP + TN)/(P + N) \n",
    "    - Most effective when generally balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error rate: misclassification rate: 1- accuracy = (FP + FN)/(P+N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sensitivity: True positive recognition rate = (TP)/P\n",
    "- Specificity: True Negative recognition rate = (TF)/F\n",
    "- Accuracy = sensitivity x (P/(P+N)) + specificity x (N/(P+N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision: measure of exactness, what % of tuples that the classifier labeled as positive actually are: TP/(TP+FP)\n",
    "- Recall: measure of completeness, what % of positive tuples did the classifier label as positive: TP/P\n",
    "- tends to be an inverse relationship, tells us how many tuples were misclassified outside of the class.\n",
    "- F measure: harmonic mean of precision and recall: F = (2 x precision x recall) / (precision + recall),  weights precision twice as much as recall\n",
    "- can change 2 to (1+ b^2)/(b^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holdout and random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Holdout method: training set = 2/3 of data, test set = 1/3 of data\n",
    "- Random sampling: repeat holdout k times, accuracy is average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-validation: randomly partition the data into k mutually exclusive subsets. At ith iteration, use Di as test, others as training\n",
    "- Leave one out, k-folds where k = number of tuples, for small sized data\n",
    "- Stratified cross validation: folds are stratified so the class dist reflects the total data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Works well with small data sets, tends to be overly optimistic\n",
    "- Samples the training tuples uniformly with replacement\n",
    "- (1-1/d)^d as d-> large, 0.368 end up not in the sample, 0.632 are in the sample\n",
    "- Repeat k times to increase accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection using statistical tests of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Null hypothesis is that M1 and M2 are the same. \n",
    "- Assume t-distribution, k-1 degrees of freedom\n",
    "- if t > z (sigma / 2) or t < -z, reject the null\n",
    "- If we reject the null, then difference between M1 and M2 is statistically significant and chose model with lower error rate\n",
    "- Formulas for pairwise comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver operator curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For visual comparison of classification models\n",
    "- Shows the trade-off between true positive rate and false positive rate\n",
    "    - The cost associated with false negative (not diagnosing a positive cancer) vs false positive (diagnosing cancer incorrectly)\n",
    "- Area under the curve is the accuracy of the model\n",
    "- Rank the tuples in order of most likely to belong to the positive class down (use naive bayes to get probability of class)\n",
    "- determine a threshold t for where the model is positive\n",
    "- Plot by moving up for true positive, right for false positive\n",
    "- (?) A model with perfect accuracy will have an area of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issues affecting model selection: accuracy, speed, robustness, scalability, interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques to improve classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble methods: use a combination of models to increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging: each training set is a bootstrap sample, sampling with replacement is used, averages the prediction over a collection of classifiers\n",
    "- Boosting and AdaBoost: weighted vote with a collection of classifiers, more accurate but risks overfitting\n",
    "- (?) Random Forest: Each classifier in the ensemble is a decision tree and is generated using a random selection of attributes at each node to determine the split. Comparable accuracy to AdaBoost, more robust to outliers and errors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dealing with class imbalanced sets: oversampling, under-sampling, threshold moving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 10: Cluster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster: a group of data objects similar to each other, dissimilar to other groups analysis is a \n",
    "- Cluster analysis: unsupervised learning, can be used as a stand alone method or as a preprocessing technique\n",
    "- Preprocessing: may be used to preprocess for classification or attribute selection or outlier detection\n",
    "- Use in applications: biology, marketing, fraud detection\n",
    "- Good clustering:\n",
    "    - high cohesiveness: within cluster\n",
    "    - distinctive between clusters\n",
    "- Requirements:\n",
    "    - scalability: clustering on all of the data, not a sample\n",
    "    - ability to deal with different data types\n",
    "    - requirements for domain knowledge\n",
    "    - ability to deal with noisy data\n",
    "- Considerations:\n",
    "    - single level vs. hierarchical\n",
    "    - completely separate or allow for overlap\n",
    "    - how to define similarity\n",
    "    - subspace clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partitioning methods: distance based, uses iterative relocation techniques to move objects from one partition to another. Works well finding spherical clusters in small-mid sized databases\n",
    "- Hierarchical methods: can be bottoms up (start with many clusters and merge) or top down (start with one cluster and split). Once a merge or step has been done, it can't be undone.\n",
    "- Density based methods: Continue to grow the cluster as long as the density is above some threshold. Can find clusters of arbitrary shape.\n",
    "- Grid based: fast processing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-means: partition objects into k groups, then repeat: compute the centroid, assign the data points to the nearest centroid.  until no change\n",
    "    - strengths: efficient O(tkn)\n",
    "    - weaknesses: often terminates at a local optimal point, only applicable to continuous n-dimensional space, need to specify k in advance, sensitive to outliers, not good with non-convex shapes, sensitive to seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-medoids: instead of taking mean, take center data point, partition the data, test a different non-medoid point, if it reduces the SSE, then reassign the medoid. PAM works well for small data sets, but it is not efficient for large data sets.\n",
    "    - CLARA is PAM but using samples\n",
    "    - (?) CLARANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Divides data into a tree of clusters, does not require K as an input\n",
    "- Agglomerative: starts with the individual vs. Divisive: starts as one cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AGNES: merge nodes that are the nearest neighbor single link (least dissimilarity)\n",
    "- DIANA: opposite of AGNES, start as one\n",
    "- Similarity is measured as the closest pair between the two clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measures of distance between clusters:\n",
    "    - single link: nearest neighbor, minimum distance\n",
    "    - complete link: maximum distance, largest distance from one element to another\n",
    "    - average: avg distance between an element in one cluster and element in another\n",
    "    - centroid: distance between centroids\n",
    "    - medoid: distance between medoids \n",
    "- Major weaknesses: cannot be undone and does not scale well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (?) BIRCH overcomes these two weaknesses, scalability and inability to undo what was previously done, however it only handles numeric data\n",
    "    - uses clustering feature to summarize the cluster, and CF tree to represent the hierarchy\n",
    "    - CF = <n,LS,SS>\n",
    "    - Can find the centroid, radius, diameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chameleon: graph based and two-phase: \n",
    "    - use graph partitioning algo to cluster objects in small sub-clusters\n",
    "    - use agglomerative hierarchical clustering algo to find genuine clusers by combining sub clusters\n",
    "    - used for clustering complex objects, however the processing cost is high 0(n^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Issues with hierarchical clustering methods: choosing a good distance measure is nontrivial, cannot have any missing attribute values, optimization goal not clear: local search\n",
    "- Probabilistic clustering: aims to overcome these using probabilistic models to measure distances between cluster\n",
    "- use a generative model, assume the data set adopts a common distribution function, find Mu and sigma that maximum likelihood the set of data points is generated\n",
    "- same efficiency, but can handle missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density-based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering based on density (local clustering)\n",
    "    - Discover clusters of arbitrary shape\n",
    "    - Can handle noise\n",
    "    - One scan, but needs a termination condition\n",
    "- DBSCAN: discover clusters of arbitrary shape, cluster: max set of density connected-points\n",
    "    - EPS: max radius of the neighborhood\n",
    "    - MinPts: min points in Eps neighborhood\n",
    "    - If it is not a core point but can be reached it is a boarder point\n",
    "    - Directly density reachable: if it belongs to the neighborhood of a core point\n",
    "    - Density reachable: chain regions to reach\n",
    "    - Computational complexity if spatial index is used, O(n log n) otherwise, O(n^2)\n",
    "    - sensitive to setting of parameters\n",
    "    - Algo: select point p, retrieve all points density reachable from p wrt eps and minPts. if core, cluster is formed, else, visit next point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OPTICS: extends DBSCAN but much less sensitive to parameter setting\n",
    "    - Density based clusters are monotonic with respect to the neighborhood threshold\n",
    "    - The deeper the valley, the denser the cluster, reachability plot\n",
    "    - core distance: the smallest distance to cover e points\n",
    "    - reachability distance: min radius that makes p density reachable from q\n",
    "        - max(core-distance, distance(q,p))\n",
    "    - O(n log n) if indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (?) DENCLUE - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- partition the data space into cells to form a grid structure, when you find a dense region in the cells, \n",
    "- Efficient and scalable, uniform but hard to handle irregular distributions, locality, curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STING: Spacial area is divided into rectangular cells at different levels of resolution to form a tree structure\n",
    "- Statistical measures for each cell. Calculate the likelihood the cell is relevant at some confidence level, only children at the relevant cells are explored\n",
    "- Query independent, complexity is O(K), k<< N\n",
    "- disadvantage: probabilistic nature may imply a loss of accuracy for query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CLIQUE: density based and grid based, connect dense units into a cluster. Starts from a low dimension\n",
    "- Start in 1D, Find dense region in each subspace and generate their minimal discriptions, then find promising candidates in 2D, repeat in levelwise manner in higher dimensional space in Apriori manner, then find the connected dense units\n",
    "- Strengths: automatically finds subspaces of the highest dimensionality as long as high density clusters exist, insensitive to the order of records, scales linearly \n",
    "- Weakness: quality of the data depends on the resolution of the gris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assessing if non-random structure exists in the data by measuring probability the data is generated by uniform data distribution\n",
    "    - Tested with Hopkins Statistic\n",
    "- Determine the number of clusters: empirical method (Sqrt(n/2)), elbow method, cross validation method\n",
    "- Measuring quality: extrinsic (supervised) vs. intrinsic (unsupervised)\n",
    "- Quality is good if it is: pure, complete, pro 'rag bag', small cluster preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
