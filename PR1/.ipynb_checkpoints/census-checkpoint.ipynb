{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{('E', 'K'): 4, ('E', 'O'): 3, ('K', 'M'): 3, ('K', 'O'): 3, ('K', 'Y'): 3}, {('E', 'K', 'O'): 3}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('E', 'K'), {'M'}],\n",
       " [('K', 'M'), {'E'}],\n",
       " [('E', 'M'), {'K'}],\n",
       " [('K',), {'E', 'M'}],\n",
       " [('E',), {'K', 'M'}],\n",
       " [('M',), {'E', 'K'}]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "test = {'T100':['M','O','N','K','E','Y'],\n",
    "        'T200':['D','O','N','K','E','Y'],\n",
    "        'T300':['M','A','K','E'],\n",
    "        'T400':['M','U','C','K','Y'], \n",
    "        'T500':['C','O','O','K','I','E']}\n",
    "    \n",
    "# data set adult.data\n",
    "url=\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# data headers from adult.names\n",
    "headers = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income',\n",
    "]\n",
    "\n",
    "# read data into pandas\n",
    "#data=pd.read_csv(url,names=headers)\n",
    "\n",
    "# Remove duplicates in transactions\n",
    "def remove_duplicates(data):\n",
    "    for key, value in data.items():\n",
    "        data[key] = list(set(value))\n",
    "    return data\n",
    "\n",
    "# Calculates support (n) from %\n",
    "def get_min_support(data, support_pct):\n",
    "    return math.ceil(len(data) * support_pct)\n",
    "    \n",
    "# Generate L(1)\n",
    "def find_frequent_1_itemsets(data):\n",
    "    # new dict to store freq\n",
    "    freq_data = {}\n",
    "    # for each transaction\n",
    "    for key, value in data.items():        \n",
    "        # for each item in the transaction\n",
    "        for i in range(len(value)):\n",
    "            # if the key is in the new dict\n",
    "            if (value[i] in freq_data):\n",
    "                # increment\n",
    "                freq_data[value[i]] += 1\n",
    "            else:\n",
    "                # else add the value to the new dict with count of one\n",
    "                freq_data[value[i]] = 1\n",
    "    # return the new dict\n",
    "    return freq_data\n",
    "\n",
    "# Prune \n",
    "def prune_itemsets(candidates, support_count):\n",
    "    # new dict\n",
    "    meet_min = {}\n",
    "    # for each candidate\n",
    "    for key, value in candidates.items():\n",
    "        # if count > support_count\n",
    "        if value >= support_count:\n",
    "            # add to the new dict\n",
    "            meet_min[key] = value\n",
    "    # return set of \n",
    "    return meet_min\n",
    "\n",
    "# Generate candidates from L(1)\n",
    "def candidate_gen_1_itemset(data):\n",
    "    # sort current candidates alphabetically\n",
    "    keys = sorted(list(data.keys()))\n",
    "    # new dict for new candidates\n",
    "    candidates = {}\n",
    "    # generate all possible combinations, add to the new dict with starting count 0 \n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i+1,len(keys)):\n",
    "            candidates[(keys[i], keys[j])] = 0    \n",
    "    return candidates\n",
    "\n",
    "# Generate candidates from L(2+) using (k-1)x(k-1) method\n",
    "def candidate_gen_k_itemset(candidates):\n",
    "    # sort current candidates alphabetically\n",
    "    keys = sorted(list(candidates.keys()))\n",
    "    k_candidates = {}\n",
    "    # for all current candidates\n",
    "    for i in range(len(keys)):\n",
    "        # for every combination of current candidates\n",
    "        for j in range(i+1,len(keys)):\n",
    "            # if (k-1) == (k-1)\n",
    "            if (keys[i][:-1]==keys[j][:-1]):\n",
    "                prefix = keys[i]\n",
    "                suffix = keys[j][-1]\n",
    "                # new candidate = k + k[-1]\n",
    "                k_candidates[(*prefix, suffix)] = 0\n",
    "    # return new candidates            \n",
    "    return k_candidates\n",
    "\n",
    "\n",
    "def find_frequent_k_itemsets(candidates, data):\n",
    "    #new dict\n",
    "    freq_data = {}\n",
    "    \n",
    "    for key_d, value_d in data.items():\n",
    "        \n",
    "        for key_c, value_c in candidates.items():\n",
    "            if set(key_c).issubset(set(value_d)):\n",
    "                if (key_c in freq_data):\n",
    "                    freq_data[key_c] += 1\n",
    "                else:\n",
    "                    freq_data[key_c] = 1\n",
    "    return freq_data\n",
    "\n",
    "\n",
    "def apriori(data, support_pct):\n",
    "    \n",
    "    # list of all min_support combinations\n",
    "    all_freq = []\n",
    "    \n",
    "    # get support count from pct\n",
    "    support_count = get_min_support(data, support_pct)\n",
    "    \n",
    "    # find frequent 1 itemsets that meet min_support and generate candidates from that\n",
    "    candidates = candidate_gen_1_itemset(prune_itemsets(find_frequent_1_itemsets(data), support_count))\n",
    "    \n",
    "    while (candidates != {}):\n",
    "        # find candidate frequency\n",
    "        L = (prune_itemsets(find_frequent_k_itemsets(candidates, data), support_count))\n",
    "        all_freq.append(L)\n",
    "        candidates = candidate_gen_k_itemset(L)\n",
    "    \n",
    "    return all_freq\n",
    "        \n",
    "def gen_association_rules(freq_item):\n",
    "    all_associations = []\n",
    "    \n",
    "    n = len(freq_item) - 1\n",
    "    \n",
    "    for i in range(n,0,-1):\n",
    "        subsets = set(itertools.combinations(freq_item,i))\n",
    "        for s in subsets:\n",
    "            l = (set(freq_item) - set(s))\n",
    "            all_associations.append([s,l])\n",
    "    return all_associations\n",
    "\n",
    "#def prune_confidence()\n",
    "\n",
    "candidates = apriori(test, .6)\n",
    "print(candidates)\n",
    "gen_association_rules(('E','K', 'M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
